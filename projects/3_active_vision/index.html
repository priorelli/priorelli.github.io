<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Active vision | Matteo Priorelli</title> <meta name="author" content="Matteo Priorelli"> <meta name="description" content="binocular depth estimation, target fixation, learning in goal-directed behavior"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.jpg?d51b3988f719277b5e0b7ca3e96e79b5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://priorelli.github.io/projects/3_active_vision/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ccb5eeb538e8a79969e344d3d49ba566"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Matteo </span>Priorelli</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Active vision</h1> <p class="post-description">binocular depth estimation, target fixation, learning in goal-directed behavior</p> </header> <article> <p align="center"> <img src="/assets/img/active_vision.png"> </p> <p>This is the project related to the paper <a href="https://www.mdpi.com/2313-7673/8/5/445" rel="external nofollow noopener" target="_blank">Active vision in binocular depth estimation: a top-down perspective</a>. It describes a hierarchical active inference model with two parallel pathways corresponding to the agent’s eyes. Each pathway performs a roto-translation of a target encoded in absolute coordinates, followed by a perspective projection. The latter is compared with the visual observation of the point in the projective plane of the eye. The proposed model can estimate the target depth through inference. Concurrently, by imposing attractors in the projective planes, an additional vergence-accommodation belief can fixate the target.</p> <p>The code can be found <a href="https://github.com/priorelli/active-vision" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="video-simulations">Video simulations</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/qcxzZHjZ0JI" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/embed/Gyv-824MQ5c" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <div class="caption"> </div> <h2 id="howto">HowTo</h2> <h3 id="start-the-simulation">Start the simulation</h3> <p>The simulation can be launched through <em>main.py</em>, either with the option <code class="language-plaintext highlighter-rouge">-m</code> for manual control, <code class="language-plaintext highlighter-rouge">-i</code> for depth estimation, <code class="language-plaintext highlighter-rouge">-r</code> for target fixation, <code class="language-plaintext highlighter-rouge">-b</code> for simultaneous estimation and fixation, or <code class="language-plaintext highlighter-rouge">-a</code> for choosing the parameters from the console. If no option is specified, the last one will be launched. For the manual control simulation, the eyes can be rotated with the keys <code class="language-plaintext highlighter-rouge">Z</code>, <code class="language-plaintext highlighter-rouge">X</code>, <code class="language-plaintext highlighter-rouge">LEFT</code>, <code class="language-plaintext highlighter-rouge">RIGHT</code>.</p> <p>Plots can be generated through <em>plot.py</em>, either with the option <code class="language-plaintext highlighter-rouge">-d</code> for the belief trajectories, or <code class="language-plaintext highlighter-rouge">-v</code> for generating a video of the simulation.</p> <p>The folder <em>reference/video/</em> contains a few videos about the applications described in the paper.</p> <p>The window is divided into three frames. The top frame shows the task from above, while the two bottom frames displays the visual observations from the perspective of the eyes. The red and orange circles correspond to the real and estimated target.</p> <h3 id="advanced-configuration">Advanced configuration</h3> <p>More advanced parameters can be manually set from <em>config.py</em>. Custom log names are set with the variable <code class="language-plaintext highlighter-rouge">log_name</code>. The number of trials, steps, and cycles can be set with the variables <code class="language-plaintext highlighter-rouge">n_trials</code>, <code class="language-plaintext highlighter-rouge">n_steps</code>, and <code class="language-plaintext highlighter-rouge">n_cycle</code> respectively.</p> <p>The parameters <code class="language-plaintext highlighter-rouge">gain_abs</code>, <code class="language-plaintext highlighter-rouge">gain_theta</code>, and <code class="language-plaintext highlighter-rouge">gain_cam</code> control the magnitude of the update step of the beliefs.</p> <p>The parameter <code class="language-plaintext highlighter-rouge">w_vis</code> controls the magnitude of the Gaussian error of the visual observations.</p> <p>The variable <code class="language-plaintext highlighter-rouge">task</code> affects the goal of the active inference agent, and can assume the following values:</p> <ol> <li> <code class="language-plaintext highlighter-rouge">infer</code>: the agent has to infer the depth of the target without rotating the eyes;</li> <li> <code class="language-plaintext highlighter-rouge">reach</code>: the agent has to fixate on the target. Note that in this case the belief of the point in absolute coordinates is set to the correct value and kept fixed throughout the trial;</li> <li> <code class="language-plaintext highlighter-rouge">both</code>: the agent has to simultaneously infer and fixate on the target. In this case, the absolute belief is free to change.</li> </ol> <p>The variable <code class="language-plaintext highlighter-rouge">context</code> specifies whether (<code class="language-plaintext highlighter-rouge">dynamic</code>) or not (<code class="language-plaintext highlighter-rouge">static</code>) the target moves. The velocity is set by <code class="language-plaintext highlighter-rouge">target_vel</code>.</p> <p>The target position can be manually set through the variable <code class="language-plaintext highlighter-rouge">target_pos</code>. If it is set to (0, 0), it will be randomly sampled at each trial.</p> <p>The parameters of the eyes, i.e., the angles, distance from the origin, and focal plane can be manually set through the variables <code class="language-plaintext highlighter-rouge">eye_angles</code>, <code class="language-plaintext highlighter-rouge">eye_lengths</code>, and <code class="language-plaintext highlighter-rouge">focal</code>.</p> <h3 id="agent">Agent</h3> <p>The script <em>simulation/inference.py</em> contains a subclass of <code class="language-plaintext highlighter-rouge">Window</code> in <em>environment/window.py</em>, which is in turn a subclass <code class="language-plaintext highlighter-rouge">pyglet.window.Window</code>. The only overriden function is <code class="language-plaintext highlighter-rouge">update</code>, which defines the instructions to run in a single cycle. Specifically, the subclass <code class="language-plaintext highlighter-rouge">Inference</code> initializes the agent and the target; during each update, it retrieves proprioceptive and visual observations through functions defined in <em>environment/window.py</em>, calls the function <code class="language-plaintext highlighter-rouge">inference_step</code> of the agent, and finally moves the eyes and the target.</p> <p>The function <code class="language-plaintext highlighter-rouge">inference_step</code> of the class <code class="language-plaintext highlighter-rouge">Agent</code> in <em>simulation/agent.py</em> executes a single update step. In particular, the function <code class="language-plaintext highlighter-rouge">get_p</code> returns visual, projective, and proprioceptive predictions. Note that the predictions of the points in the reference frames relative to the eyes are computed implicitly through the function <code class="language-plaintext highlighter-rouge">get_rel</code>. The function <code class="language-plaintext highlighter-rouge">get_i</code> returns future beliefs depending on the agent’s intentions, e.g., reach a point in absolute coordinates, a point projected in the eye planes, or rotate by a specific angle. Functions <code class="language-plaintext highlighter-rouge">get_e_g</code> and <code class="language-plaintext highlighter-rouge">get_e_mu</code> compute sensory and dynamics prediction errors, respectively. The function <code class="language-plaintext highlighter-rouge">get_likelihood</code> backpropagates the sensory errors toward the beliefs, calling the function <code class="language-plaintext highlighter-rouge">grad_cam</code> to compute the gradients of the absolute belief and the vergence-accommodation angles. Note that this function could infer the belief over the eye lengths <code class="language-plaintext highlighter-rouge">mu_len</code> through the variable <code class="language-plaintext highlighter-rouge">grad_len</code>. Note also that the absolute belief is affected by the likelihoods of both eyes, and letting it depend only on a single contribution is the equivalent of closing an eye. Finally, the function <code class="language-plaintext highlighter-rouge">mu_dot</code> computes the total belief updates, also considering the backward and forward errors <code class="language-plaintext highlighter-rouge">E_mu</code> of the dynamics functions.</p> <p>The function <code class="language-plaintext highlighter-rouge">set_mode</code> is used to control the action-perception cycles for simultaneous inference and fixation.</p> <p>Useful trajectories computed during the simulations are stored through the class <code class="language-plaintext highlighter-rouge">Log</code> in <em>environment/log.py</em>.</p> <p>Note that all the variables are normalized between -1 and 1 to ensure that every contribution to the belief updates has the same magnitude.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Priorelli2023c" class="col-sm-8"> <div class="title">Active Vision in Binocular Depth Estimation: A Top-Down Perspective</div> <div class="author"> <em>Matteo Priorelli</em>, Giovanni Pezzulo, and Ivilin Peev Stoianov</div> <div class="periodical"> <em>Biomimetics</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Matteo Priorelli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>